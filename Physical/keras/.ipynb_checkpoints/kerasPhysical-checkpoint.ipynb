{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"theano\"\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import PIL\n",
    "from PIL import Image, ImageFilter\n",
    "import requests\n",
    "from io import BytesIO\n",
    "\n",
    "import imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rames\\Documents\\GitHub\\ai-personas\\Personas\\personaDefinition_pb2.py\n",
      "imageInput\n",
      "(50L, 50L)\n",
      "http://www.loc.gov/rr/scitech/subjectguides/images/tesla-new.jpg\n",
      "(50L, 50L)\n",
      "https://static1.squarespace.com/static/51cdd10de4b08819bd7bc9b4/525d89c2e4b0f8245cabfc96/53cfd660e4b089801fe7e3d9/1458152617516/SciSource_BK0929.jpg?format=750w\n",
      "(50L, 50L)\n",
      "https://static1.squarespace.com/static/51cdd10de4b08819bd7bc9b4/525d89c2e4b0f8245cabfc96/53cfd28be4b0c47638e2f540/1458152692173/SciSource_BN4337.jpg?format=750w\n",
      "(50L, 50L)\n",
      "http://www.loc.gov/rr/scitech/subjectguides/images/tesla-new.jpg\n",
      "(50L, 50L)\n",
      "https://static1.squarespace.com/static/51cdd10de4b08819bd7bc9b4/525d89c2e4b0f8245cabfc96/53cfd660e4b089801fe7e3d9/1458152617516/SciSource_BK0929.jpg?format=750w\n",
      "(50L, 50L)\n",
      "https://static1.squarespace.com/static/51cdd10de4b08819bd7bc9b4/525d89c2e4b0f8245cabfc96/53cfd28be4b0c47638e2f540/1458152692173/SciSource_BN4337.jpg?format=750w\n",
      "(3L, 1L, 50L, 50L)\n",
      "(3L, 1L, 50L, 50L)\n",
      "1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: expected activation_6 to have shape (None, 5000L, 50L, 50L) but got array with shape (3L, 1L, 50L, 50L)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-b185d5e3f55b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[1;31m#output_data = model.predict(x_action_data)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mrunPersona\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpersona\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-b185d5e3f55b>\u001b[0m in \u001b[0;36mrunPersona\u001b[0;34m(persona)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mnb_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     model.fit(x_train_data, y_train_data, batch_size=batch_size, nb_epoch=nb_epoch,\n\u001b[0;32m--> 105\u001b[0;31m           verbose=1, validation_data=(x_train_data, y_train_data))\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[1;31m#output for given input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rames\\Anaconda2\\lib\\site-packages\\keras\\models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32mC:\\Users\\rames\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[1;31m# prepare validation data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\rames\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1031\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m                                    \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m                                    exception_prefix='model target')\n\u001b[0m\u001b[1;32m   1034\u001b[0m         sample_weights = standardize_sample_weights(sample_weight,\n\u001b[1;32m   1035\u001b[0m                                                     self.output_names)\n",
      "\u001b[0;32mC:\\Users\\rames\\Anaconda2\\lib\\site-packages\\keras\\engine\\training.pyc\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    122\u001b[0m                             \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                             \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    125\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: expected activation_6 to have shape (None, 5000L, 50L, 50L) but got array with shape (3L, 1L, 50L, 50L)"
     ]
    }
   ],
   "source": [
    "#persona definition\n",
    "persona_definition_path = os.path.abspath(os.path.join('..', '..', 'Personas', 'personaDefinition_pb2.py'))\n",
    "print (persona_definition_path)\n",
    "#import persona proto modules\n",
    "persona = imp.load_source('Persona', persona_definition_path).Persona()\n",
    "\n",
    "#load persona\n",
    "personaName = \"Khandhasamy\"\n",
    "personaCategory = ['Artist', 'Portraits', 'sketchToGreyImage'] \n",
    "personaPath = os.path.join('..', '..', 'Personas')\n",
    "for category in personaCategory:\n",
    "    personaPath = os.path.join(personaPath, category)\n",
    "\n",
    "#persona file\n",
    "persona_abs_path = os.path.abspath(os.path.join(personaPath, personaName , personaName + '.bin'))\n",
    "f = open(persona_abs_path, \"rb\")\n",
    "persona.ParseFromString(f.read())\n",
    "\n",
    "def getInformationSource(persona, layer):\n",
    "    for environment in persona.age.environments:\n",
    "        for information in environment.informations:\n",
    "            for informationConnectedLayerName in information.connectedLayerName:\n",
    "                if (layer.layerName == informationConnectedLayerName):\n",
    "                    #print (information.informationSource)\n",
    "                    return getExtractor(information)\n",
    "\n",
    "def getExtractor(information):\n",
    "    #information definition\n",
    "    information_definition_path = os.path.abspath(os.path.join('..', '..', 'Environment', information.informationSource + \"_pb2.py\"))\n",
    "    informationModule = imp.load_source('Information', information_definition_path).Information()  \n",
    "    information_file_path = os.path.abspath(os.path.join('..', '..', 'Environment', information.informationSource + \".bin\"))\n",
    "    informationFile = open(information_file_path, \"rb\")\n",
    "    informationModule.ParseFromString(informationFile.read())\n",
    "    #load extractor\n",
    "    information_extractor_path = os.path.abspath(os.path.join('..', '..', 'Environment/Informations/Process/Extract/' + informationModule.extractor + \".py\"))\n",
    "    extractorModule = imp.load_source(informationModule.extractor, information_extractor_path).ImageURLExtractor(informationModule)\n",
    "    #print (informationModule.extractor)\n",
    "    return extractorModule\n",
    "\n",
    "LAYER_CONVOLUTION = \"layerConvolution\"\n",
    "LAYER_ACTIVATION = \"layerActivation\"\n",
    "LAYER_DROPOUT = \"layerDropout\"\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "def runPersona(persona):\n",
    "    x_train_data = None\n",
    "    y_train_data = None\n",
    "    x_action_data = None\n",
    "    input_shape = None\n",
    "    batch_size = None\n",
    "\n",
    "    for dna in persona.DNAs:\n",
    "        for inputLayer in dna.inputs:\n",
    "            print (inputLayer.layerName)\n",
    "            inputTransform = inputLayer.inputTransform\n",
    "            #get information source\n",
    "            information = getInformationSource(persona, inputLayer)\n",
    "\n",
    "            x_train_data = information.getData(inputTransform)\n",
    "            #x_action_data = information.get_data(inputTransform)\n",
    "            print (\"input\")\n",
    "            \n",
    "        for layer in dna.layers:\n",
    "            if (LAYER_CONVOLUTION == layer.WhichOneof(\"SubLayer\")):\n",
    "                nb_filters = layer.layerConvolution.filters\n",
    "                convDimension = layer.layerConvolution.convolutionDimension\n",
    "                borderMode = layer.layerConvolution.borderMode\n",
    "                kernelSize = layer.layerConvolution.kernelSize\n",
    "                inputShape = layer.layerConvolution.inputShape\n",
    "                if K.image_dim_ordering() == 'th':\n",
    "                    conv_input_shape = (inputShape[0], inputShape[1], inputShape[2])\n",
    "                else:\n",
    "                    conv_input_shape = (inputShape[1], inputShape[2], inputShape[0])\n",
    "                if convDimension == 2:\n",
    "                    model.add(Convolution2D(nb_filters*50, kernelSize[0], kernelSize[1],\n",
    "                        border_mode=borderMode,\n",
    "                        input_shape=conv_input_shape))\n",
    "                    print (\"conv2D\")\n",
    "            if (LAYER_ACTIVATION == layer.WhichOneof(\"SubLayer\")):  \n",
    "                activationType = layer.layerActivation.activationType\n",
    "                model.add(Activation(activationType))\n",
    "                print (\"activation\")\n",
    "            if (LAYER_DROPOUT == layer.WhichOneof(\"SubLayer\")):\n",
    "                dropoutPercentage = layer.layerDropout.dropPercentage\n",
    "                model.add(Dropout(dropoutPercentage))\n",
    "                print (\"dropout\")\n",
    "                \n",
    "        for outputLayer in dna.outputs:\n",
    "            #print (outputLayer.layerName)\n",
    "            inputTransform = outputLayer.inputTransform\n",
    "            #get information source\n",
    "            information = getInformationSource(persona, outputLayer)\n",
    "            #train data\n",
    "            y_train_data = information.getData(inputTransform)\n",
    "            print (\"output\")\n",
    "    \n",
    "    #compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adadelta')\n",
    "    \n",
    "    print (x_train_data.shape)\n",
    "    print (y_train_data.shape)\n",
    "\n",
    "    \n",
    "    #learning\n",
    "    batch_size = persona.age.learningBatchSize\n",
    "    print (batch_size)\n",
    "    nb_epoch = 50\n",
    "    model.fit(x_train_data, y_train_data, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(x_train_data, y_train_data))\n",
    "    \n",
    "    #output for given input\n",
    "    #output_data = model.predict(x_action_data)\n",
    "    \n",
    "runPersona(persona)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 50\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 100, 100\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 1\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# X_train_data = np.random.random((6,1, img_rows, img_cols))\n",
    "# Y_train_data = np.random.random((6,1, img_rows, img_cols))\n",
    "\n",
    "# with open(\"../../Environment/Informations/Category/Portraits/trainImages/images.txt\") as f:\n",
    "#     infoFileContents = f.readlines()\n",
    "#     for l in range(len(infoFileContents)):\n",
    "#         infoLine = infoFileContents[l].lstrip().rstrip()\n",
    "        \n",
    "#         print (infoLine)\n",
    "#         response = requests.get(infoLine)\n",
    "#         trainInpImg = Image.open(BytesIO(response.content))\n",
    "#         trainInpGrayImg = trainInpImg.convert(\"L\")\n",
    "#         trainInpGrayImg = trainInpGrayImg.resize((img_rows,img_cols), PIL.Image.ANTIALIAS)\n",
    "#         trainInpGreyImgArray = np.asarray(trainInpGrayImg, dtype=np.float32)\n",
    "#         Y_train_data[l,0,:,:] = trainInpGreyImgArray\n",
    "        \n",
    "#         trainInpEdgeImg = trainInpImg.convert(\"L\")\n",
    "#         trainInpEdgeImg = trainInpEdgeImg.resize((img_rows,img_cols), PIL.Image.ANTIALIAS)\n",
    "#         trainInpEdgeImg = trainInpEdgeImg.filter(ImageFilter.FIND_EDGES)\n",
    "\n",
    "#         trainInpEdgeImgArray = np.asarray(trainInpEdgeImg, dtype=np.float32)\n",
    "#         X_train_data[l,0,:,:] = trainInpEdgeImgArray\n",
    "\n",
    "# X_test_data = np.random.random((2,1, img_rows, img_cols))\n",
    "# Y_test_data = np.random.random((2,1, img_rows, img_cols))        \n",
    "\n",
    "# with open(\"../../Environment/Informations/Category/Portraits/testImages/images.txt\") as f:\n",
    "#     infoFileContents = f.readlines()\n",
    "#     for l in range(len(infoFileContents)):\n",
    "#         infoLine = infoFileContents[l].lstrip().rstrip()\n",
    "        \n",
    "#         print (infoLine)\n",
    "#         response = requests.get(infoLine)\n",
    "#         trainInpImg = Image.open(BytesIO(response.content))\n",
    "#         trainInpGrayImg = trainInpImg.convert(\"L\")\n",
    "#         trainInpGrayImg = trainInpGrayImg.resize((img_rows,img_cols), PIL.Image.ANTIALIAS)\n",
    "#         trainInpGreyImgArray = np.asarray(trainInpGrayImg, dtype=np.float32)\n",
    "#         Y_test_data[l,0,:,:] = trainInpGreyImgArray\n",
    "        \n",
    "#         trainInpEdgeImg = trainInpImg.convert(\"L\")\n",
    "#         trainInpEdgeImg = trainInpEdgeImg.resize((img_rows,img_cols), PIL.Image.ANTIALIAS)\n",
    "#         trainInpEdgeImg = trainInpEdgeImg.filter(ImageFilter.FIND_EDGES)\n",
    "#         trainInpEdgeImgArray = np.asarray(trainInpEdgeImg, dtype=np.float32)\n",
    "#         X_test_data[l,0,:,:] = trainInpEdgeImgArray\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_data = X_train_data.astype('float32')\n",
    "X_train_data /= 255\n",
    "print('X_train shape:', X_train_data.shape)\n",
    "print(X_train_data.shape[0], 'train samples')\n",
    "Y_train_data = Y_train_data.astype('float32')\n",
    "Y_train_data /= 255\n",
    "print('Y_train shape:', Y_train_data.shape)\n",
    "print(Y_train_data.shape[0], 'train samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if K.image_dim_ordering() == 'th':\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters*100, kernel_size[0], kernel_size[1],\n",
    "                        border_mode='same',\n",
    "                        input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Convolution2D(nb_filters*100, kernel_size[0], kernel_size[1],\n",
    "                         border_mode='same',\n",
    "                         input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Convolution2D(nb_filters*100, kernel_size[0], kernel_size[1],\n",
    "                         border_mode='same',\n",
    "                         input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Convolution2D(nb_filters*100, kernel_size[0], kernel_size[1],\n",
    "                         border_mode='same',\n",
    "                         input_shape=input_shape))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Convolution2D(nb_filters, kernel_size[0], kernel_size[1],\n",
    "                         border_mode='same',\n",
    "                         input_shape=input_shape))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_data, Y_train_data, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "          verbose=1, validation_data=(X_test_data, Y_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(X_train_data, Y_train_data, verbose=0)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_data = model.predict(X_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plot_image = np.concatenate((X_train_data[0][0]*255, Y_train_data[0][0]*255), axis=1)\n",
    "plt.imshow(X_test_data[0][0]*255, cmap = cm.Greys_r)\n",
    "plt.show()\n",
    "plt.imshow(pred_data[0][0]*255, cmap = cm.Greys_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainInpGrayImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainInpEdgeImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prdImg = Image.fromarray(pred_data[0][0], 'L')\n",
    "prdImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
